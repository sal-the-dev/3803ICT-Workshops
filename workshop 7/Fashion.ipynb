{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m0ww1p87nRo"
      },
      "source": [
        "# Image Classification by MLP - Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoPHvo1i7nRp"
      },
      "source": [
        "In this exercise, we will try to use a neural network on a simple classification task: classifying images of clothes into 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GuR1mdR7nRp"
      },
      "source": [
        "We will first download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpFWXtqR7nRq",
        "outputId": "0d219e0e-98eb-4d8c-b750-bac5ae1b94c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "#TODO: load dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#TODO: Resample the dataset if needed\n",
        "# X_train = ...\n",
        "# y_train = ...\n",
        "# X_test = ...\n",
        "# y_test = ...\n",
        "\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuV4OcLf7nRr"
      },
      "source": [
        "This dataset contains 10 classes:\n",
        "* 0:\tT-shirt/top\n",
        "* 1:\tTrouser\n",
        "* 2:\tPullover\n",
        "* 3:\tDress\n",
        "* 4:\tCoat\n",
        "* 5:\tSandal\n",
        "* 6:\tShirt\n",
        "* 7:\tSneaker\n",
        "* 8:\tBag\n",
        "* 9:\tAnkle boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAt0Seg27nRr"
      },
      "source": [
        "Now begin by exploring the data. Try to display some images with the associated label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "E6ITK5pO7nRr",
        "outputId": "5e914032-2439-47d0-a588-9d686f3ebaca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: Explore the data, display some input images\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "label_class = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "\n",
        "# np.random.seed(0)\n",
        "idx = np.random.randint(X_train.shape[0])\n",
        "\n",
        "plt.imshow(X_train[idx], cmap=\"gray_r\")\n",
        "plt.title(label_class[y_train[idx]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEyj591Q7nRt"
      },
      "source": [
        "**Before going further**: what methods could you use to perform such a classification task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpDTQgkC7nRt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jts6ueqy7nRu"
      },
      "source": [
        "The first method you will try is using neural networks. First step is the data preparation: data rescaling, label preparation.\n",
        "\n",
        "Hint: you can use the Keras function `to_categorical`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XLPxqAcX7nRu"
      },
      "outputs": [],
      "source": [
        "# TODO: Make the data preparation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train, num_classes=len(label_class))\n",
        "y_test_cat = to_categorical(y_test, num_classes=len(label_class))\n",
        "\n",
        "X_train_norm = X_train / 255\n",
        "X_test_norm = X_test / 255\n",
        "\n",
        "# TODO: reshape the image data (2D array) into input 1D array for a neural network\n",
        "X_train_norm = X_train_norm.reshape(X_train_norm.shape[0], np.prod(X_train_norm.shape[1:]))\n",
        "X_test_norm = X_test_norm.reshape(X_test_norm.shape[0], np.prod(X_test_norm.shape[1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYMKEhJf7nRu"
      },
      "source": [
        "Next step: model building with Keras. Build your neural network architecture. At first, I would recommend a light architecture: no more than 2 hidden layers, with about 10 units per layer. Put that model into a function, so that you can reuse it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m37GYIMr7nRv",
        "outputId": "6a641eab-943b-4a8c-92ce-8e838c1957ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,070\n",
            "Trainable params: 8,070\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# TODO: Build your model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def my_model(input_dim):\n",
        "    # Create the Sequential object\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add 2 dense layers with 10 neurons each using sigmoid or relu activation\n",
        "    model.add(Dense(10, input_dim=input_dim, activation=\"sigmoid\"))\n",
        "    model.add(Dense(10, activation=\"sigmoid\"))\n",
        "    \n",
        "    # Add the output layer with one unit: the predicted result\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    return model\n",
        "  \n",
        "my_model(X_train_norm.shape[1]).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgV_FysZ7nRv"
      },
      "source": [
        "Now compile and fit your model on your training data. Since this is a multiclass classification, the loss is not `binary_crossentropy` anymore, but `categorical_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFsms8Xx7nRv",
        "outputId": "836095da-13be-48c2-f83d-ae8e4a908549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 3s 4ms/step - loss: 1.8996 - accuracy: 0.3458\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 1.4224 - accuracy: 0.5196\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.1639 - accuracy: 0.6597\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.9551 - accuracy: 0.7479\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.7654 - accuracy: 0.7790\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.6504 - accuracy: 0.7971\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5837 - accuracy: 0.8134\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.8250\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5126 - accuracy: 0.8340\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4892 - accuracy: 0.8403\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4696 - accuracy: 0.8450\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4545 - accuracy: 0.8479\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4405 - accuracy: 0.8515\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4304 - accuracy: 0.8547\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4227 - accuracy: 0.8559\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4142 - accuracy: 0.8586\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4091 - accuracy: 0.8599\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4031 - accuracy: 0.8615\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3976 - accuracy: 0.8639\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3936 - accuracy: 0.8647\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3901 - accuracy: 0.8665\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3861 - accuracy: 0.8668\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3833 - accuracy: 0.8679\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3807 - accuracy: 0.8688\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3784 - accuracy: 0.8696\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3756 - accuracy: 0.8701\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3728 - accuracy: 0.8711\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3711 - accuracy: 0.8714\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3675 - accuracy: 0.8723\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3664 - accuracy: 0.8733\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3648 - accuracy: 0.8729\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3627 - accuracy: 0.8745\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3615 - accuracy: 0.8750\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3597 - accuracy: 0.8761\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3584 - accuracy: 0.8749\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3560 - accuracy: 0.8766\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3552 - accuracy: 0.8767\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3537 - accuracy: 0.8767\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3521 - accuracy: 0.8780\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3518 - accuracy: 0.8773\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3498 - accuracy: 0.8781\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3489 - accuracy: 0.8781\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3472 - accuracy: 0.8797\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3470 - accuracy: 0.8789\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3457 - accuracy: 0.8793\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3440 - accuracy: 0.8796\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3433 - accuracy: 0.8802\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3418 - accuracy: 0.8808\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3416 - accuracy: 0.8807\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3395 - accuracy: 0.8814\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3392 - accuracy: 0.8811\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3374 - accuracy: 0.8819\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3367 - accuracy: 0.8817\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3364 - accuracy: 0.8822\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3351 - accuracy: 0.8825\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3342 - accuracy: 0.8834\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.8837\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3323 - accuracy: 0.8834\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8827\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8850\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8847\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8837\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3287 - accuracy: 0.8852\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.8843\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3275 - accuracy: 0.8849\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3259 - accuracy: 0.8857\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3257 - accuracy: 0.8856\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3265 - accuracy: 0.8849\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3246 - accuracy: 0.8862\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8865\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3243 - accuracy: 0.8857\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3235 - accuracy: 0.8870\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3214 - accuracy: 0.8869\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3215 - accuracy: 0.8874\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3210 - accuracy: 0.8876\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3207 - accuracy: 0.8875\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3201 - accuracy: 0.8867\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3195 - accuracy: 0.8881\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3196 - accuracy: 0.8876\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3171 - accuracy: 0.8889\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3179 - accuracy: 0.8883\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3178 - accuracy: 0.8884\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3171 - accuracy: 0.8887\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3165 - accuracy: 0.8881\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3155 - accuracy: 0.8895\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3159 - accuracy: 0.8889\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3151 - accuracy: 0.8893\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3151 - accuracy: 0.8888\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3133 - accuracy: 0.8897\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3133 - accuracy: 0.8895\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3134 - accuracy: 0.8892\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3130 - accuracy: 0.8900\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3136 - accuracy: 0.8895\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3126 - accuracy: 0.8906\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3115 - accuracy: 0.8906\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3108 - accuracy: 0.8907\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3106 - accuracy: 0.8911\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.3103 - accuracy: 0.8909\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3100 - accuracy: 0.8919\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3084 - accuracy: 0.8925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb3d9f75600>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "#https://stackoverflow.com/questions/53014306/error-15-initializing-libiomp5-dylib-but-found-libiomp5-dylib-already-initial\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
        "\n",
        "# TODO: Compile and fit your model\n",
        "model = my_model(X_train_norm.shape[1])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_norm, y_train_cat, epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2-cMKHn7nRv"
      },
      "source": [
        "Once your model has been trained, compute the accuracy (and other metrics if you want) on the train and test dataset.\n",
        "\n",
        "Be careful, Keras returns softmax output (so an array of 10 values between 0 and 1, for which the sum is equal to 1). To compute correctly the accuracy, you have to convert that array into a categorical array with zeros and a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S2LoeQ57nRv",
        "outputId": "4e37b4b6-f7c0-4de6-c53f-d76d0b55d92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on train with NN: 0.8904833197593689\n",
            "accuracy on test with NN: 0.8525000214576721\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute the accuracy of your model\n",
        "print('accuracy on train with NN:', model.evaluate(X_train_norm, y_train_cat, verbose=0)[1])\n",
        "print('accuracy on test with NN:', model.evaluate(X_test_norm, y_test_cat, verbose=0)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7m48hpg7nRx"
      },
      "source": [
        "What do you think of those results? Can you improve it by changing the number of layers? Of units per layer? The number of epochs? The activation functions?\n",
        "\n",
        "You should try!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahY4j5N7nRx"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uGIvn3B7nRx"
      },
      "source": [
        "In order to compare your results with more traditional machine learning methods, you will do this work with another method: a PCA followed by a classification model (of your choice). Of course, you can perform hyperparameter optimization using a gridsearch on that model!\n",
        "\n",
        "Fit your model and display the performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "encR-Woz7nRx"
      },
      "outputs": [],
      "source": [
        "# TODO: Redo the classification with PCA and classification model\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.9)\n",
        "\n",
        "pca.fit(X_train_norm)\n",
        "X_train_pca = pca.transform(X_train_norm)\n",
        "X_test_pca = pca.transform(X_test_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DinnzKQf7nRx",
        "outputId": "1b9e934e-b22b-4154-ce36-cade97f886cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score with RF on train 1.0\n",
            "score with RF on test 0.8621\n"
          ]
        }
      ],
      "source": [
        "# TODO: use any classifier you want\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(X_train_pca, y_train)\n",
        "\n",
        "print('score with RF on train', rf.score(X_train_pca, y_train))\n",
        "print('score with RF on test', rf.score(X_test_pca, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agVHitLI7nRx"
      },
      "source": [
        "Are the performances different? Can you explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtcZXM0o7nRx"
      },
      "source": [
        "If you still have time, you could try to use scikit-learn's `Pipeline` to perform the hyperparameter optimization jointly on the PCA and the classification model. This might improve your performances."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}